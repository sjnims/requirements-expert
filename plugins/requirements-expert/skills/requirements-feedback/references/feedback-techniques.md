# Feedback Collection Techniques

Detailed methods and guidance for collecting feedback from different sources throughout the requirements lifecycle.

---

## User Research

### Methods

- **User interviews:** One-on-one conversations about problems and needs
- **Usability testing:** Observe users attempting tasks with prototypes or implementations
- **Surveys:** Structured questions to larger user groups about feature priorities
- **Analytics:** Track and analyze actual usage patterns

### When to Use

- **Vision validation:** Does the problem resonate with target users?
- **Epic prioritization:** What capabilities matter most to users?
- **Story refinement:** Does the proposed solution work for users?
- **Post-launch:** Are we delivering expected value?

### Incorporating Results

- Update requirements based on validated learnings
- Adjust priorities based on actual user behavior
- Create new requirements for discovered needs
- Remove or deprioritize unused features

---

## Stakeholder Reviews

### Methods

- **Review meetings:** Regular scheduled reviews with key stakeholders
- **Async reviews:** GitHub issue comments in GitHub Projects for distributed feedback
- **Roadmap planning:** Sessions to align on strategic priorities
- **Business alignment:** Check-ins to verify requirements support business goals

### When to Use

- **Vision and epic level:** Strategic alignment decisions
- **Major priority decisions:** Resource allocation and sequencing
- **Scope change requests:** When requirements need modification
- **Resource allocation:** When capacity affects what can be built

### Incorporating Results

- Update strategic alignment notes in vision/epics
- Adjust priorities based on business needs
- Add or remove requirements based on strategic shifts
- Document stakeholder decisions for future reference

---

## Team Feedback

### Methods

- **Backlog refinement:** Regular sessions to review and clarify upcoming stories
- **Sprint retrospectives:** Reflect on what worked and what didn't
- **Code review comments:** Technical insights from implementation
- **Technical spikes:** Findings from exploratory investigation

### When to Use

- **Story and task creation:** Clarity and feasibility validation
- **Implementation challenges:** When development reveals issues
- **Technical feasibility:** Questions about approach or complexity
- **Process improvements:** Identifying patterns for better requirements

### Incorporating Results

- Clarify ambiguous requirements
- Split overly complex requirements
- Add technical constraints or considerations
- Improve future requirements based on lessons learned
- Update estimates based on implementation reality

---

## Automated Feedback

### Methods

- **Usage analytics:** Track feature adoption and usage patterns
- **Error monitoring:** Identify bugs, crashes, and issues in production
- **Performance metrics:** Measure speed, reliability, and scalability
- **A/B testing:** Compare alternatives empirically with real users

### When to Use

- **Post-launch validation:** Verify features work as expected
- **Feature usage analysis:** Understand what users actually use
- **Performance requirements:** Validate non-functional requirements
- **Hypothesis testing:** Empirically test assumptions

### Incorporating Results

- Validate or invalidate initial assumptions
- Identify unused features to remove or redesign
- Discover high-value features to enhance further
- Update success metrics based on production reality
- Create data-driven prioritization decisions

---

## Choosing the Right Technique

| Situation | Recommended Techniques |
|-----------|------------------------|
| Validating problem/vision | User interviews, stakeholder reviews |
| Prioritizing features | Surveys, stakeholder reviews, analytics |
| Clarifying requirements | Team feedback, backlog refinement |
| Testing solutions | Usability testing, A/B testing |
| Measuring success | Analytics, performance metrics |
| Continuous improvement | Retrospectives, error monitoring |

---

## Best Practices for All Techniques

### Preparation

- Define what you want to learn before gathering feedback
- Prepare specific questions or hypotheses to validate
- Identify the right participants or data sources
- Allocate adequate time for feedback collection and analysis

### Collection

- Create a safe space for honest feedback
- Ask open-ended questions before leading questions
- Document feedback as it's received
- Look for patterns across multiple sources

### Analysis

- Distinguish opinions from validated facts
- Prioritize feedback by impact and confidence
- Identify themes and patterns
- Validate surprising feedback with additional sources

### Action

- Update requirements promptly while context is fresh
- Communicate changes to relevant stakeholders
- Close the loop with those who provided feedback
- Document the rationale for decisions made
